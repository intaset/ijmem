<!DOCTYPE html>

<html lang="en"><head>

<meta charset="utf-8">

<meta http-equiv="X-UA-Compatible" content="IE=edge">

<meta name="viewport" content="width=device-width, initial-scale=1">
<title>IJMEM - Assisting and Guiding Visually Impaired in Indoor Environments</title>
<link href='http://fonts.googleapis.com/css?family=PT+Sans:300,400,400italic,500,600,700,700italic&amp;subset=latin,greek-ext,cyrillic,latin-ext,greek,cyrillic-ext,vietnamese' rel='stylesheet' type='text/css' />

<link href='http://fonts.googleapis.com/css?family=Antic+Slab:300,400,400italic,500,600,700,700italic&amp;subset=latin,greek-ext,cyrillic,latin-ext,greek,cyrillic-ext,vietnamese' rel='stylesheet' type='text/css' />

<link rel="stylesheet" href="../css/style.css" />

<link href="../css/bootstrap.css" rel="stylesheet">
<link href="../images/icon.ico" rel="shortcut icon" type="image/x-icon">
<!--Goole Analytics Most Visited Article Code-->
<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-45437411-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>


<!-- Include Most Visited Articles JS Code -->
<script type="text/javascript" src="mostvisited.js"></script>

<!-- Include SHARE THIS js sources -->
<script type="text/javascript">var switchTo5x=true;</script>
<script type="text/javascript" src="http://w.sharethis.com/button/buttons.js"></script>
<script type="text/javascript">stLight.options({publisher: "8f49e037-5155-4595-abc0-d069deae6488", doNotHash: true, doNotCopy: false, hashAddressBar: false, onhover:false});</script>
</head>

<body>

<div class="wrapper">

  <div class="header">

    <div class="container">

      <div class="row">

        <header id="header">
          <div class="logo col-sm-4"><a href="http://avestia.com"><img src="/images/logo.png" alt="Journal" class="img" /></a></div>
          <div class="threemenu col-sm-5">
            <div class="row"><a class="link col-sm-4" href="http://avestia.com"><img src="/images/home.jpg" class="img"></a> <a class="link col-sm-4" href="http://avestia.com/journals/"><img src="/images/journals.jpg" class="img"></a> <a class="link col-sm-4" href="http://amss.avestia.com"><img src="/images/submit.jpg" class="img"></a></div>
          </div>
          <div class="searching col-sm-3">
<div class="sharethis">Share this page:<br/>
<span class='st_email'></span>
<span class='st_facebook'></span>
<span class='st_twitter'></span>
<span class='st_googleplus'></span>
<span class='st_linkedin'></span>
</div>
            
         <div class="search_wrapper">
         <form class="search" action="../results" method="get">
              <fieldset>
                <span class="text">
                <input name="q" id="q" type="text" value="" placeholder="Search..." /><button class="search_top_button">GO</button>
                </span>

              </fieldset>
            </form>
            
            </div>
          </div>
          <div class="col-lg-12"><div class="avada-row">

            <nav id="nav" class="nav-holder">

              <div role="navigation" class="navbar navbar-default">

                <div class="container-fluid">

                  <div class="navbar-header">
                  
                    <button data-target=".navbar-collapse" data-toggle="collapse" class="navbar-toggle" type="button"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar"></span> <span class="icon-bar"></span> <span class="icon-bar"></span> </button>
<span class="navbar-brand" href="#">Menu</span>
                  </div>

                  <div class="navbar-collapse collapse">

                    <ul class="nav navbar-nav">			
						<li><a href="/">Journal Home</a></li><span class="menu-line">|</span>
                        <li><a href="../aims/">Aims and Scopes</a></li><span class="menu-line">|</span>
                        <li><a href="../charges/">Article Processing Charges</a></li><span class="menu-line">|</span>
                        <li><a href="../guidelines/">Author Guidelines</a></li><span class="menu-line">|</span>
                        <li><a href="../board/">Editorial Board</a></li><br>
                        <li><a href="../volumes/">Past Volumes</a></li><span class="menu-line">|</span>
                        <li><a href="../current/">Current Volume</a></li><span class="menu-line">|</span>
                        <li><a href="../propose/">Propose a Special Topic</a></li><span class="menu-line">|</span>
                        <li><a href="../register/">Receive Updates</a></li><span class="menu-line">|</span>
                        <li><a href="../contact/">Contact Us</a></li>




                    </ul>

                  </div>

                  <!--/.nav-collapse -->

                </div>

                <!--/.container-fluid -->

              </div>

            </nav>

            </div>

          </div>
          <div class="clearfix"></div>

        </header>

      </div>

    </div>
<div class="clearfix"></div>
  </div>

  <div class="slider">

  <div class="container">

  <div class="row"><div class="col-lg-12"><img class="img2" src="../images/international.jpg" alt="img"></div></div>

  </div>

  </div>

  <div class="container">
  <div class="clearfix"></div>
  <div class="row">

  <div class="col-lg-12 text"><div align=center>
<div class="table">

    <table class=MsoTableGrid border=0 cellspacing=0 cellpadding=0 style='border:none; width: 100%'>
         <tr>
                <td>Volume 1, Issue 2 - Year 2012 - Pages 9-14</td>
                <td align="right"><a href="PDF/002.pdf">View PDF (Full-text)</a></td>
        </tr>
         <tr>
                <td>DOI: 10.11159/ijmem.2012.002</td>
                <td align="right"><a href="#references">Linked References</a></td>
        </tr>
		<tr>
	<td colspan="2">ISSN: 1929-2724</td>
	</tr>
     </table>
     
</div>
</div>

<br>

<p align="center" style="font-size:22pt;">Assisting and Guiding Visually Impaired in Indoor Environments</p>
	<p align="center"><strong>Genci Capi</strong>
    <br>University of Toyama
Gofuku 3910, Toyama, Japan<br>
capi@eng.u-toyama.ac.jp



</p>

<p>&nbsp;</p>
<p>&nbsp;</p>

<p align="justify"><i><strong>Abstract- </strong> This paper presents a new intelligent robotic system to assist or guide visually impaired people.  The robotic system, which is equipped with a visual sensor, laser range finders, speaker, gives visually impaired people information about the environment around them. The laser data are analyzed using the clustering technique, making it possible to detect obstacles, steps and stairs. The PC analysis the sensors data and send information to the visually impaired people by natural language or beep signal. In the guide mode, the robot navigates controlled by evolved neural networks. By selecting the target location, the robot starts guiding the user toward the goal. The robot switches between neural controllers based on the robot location as it navigates in the environment. The usefulness of the proposed system is examined experimentally. </span></i></p>

<p align="left"><i><strong>Keywords:</strong></i> Visually Impaired People, Guide Robot, Assistive Robot, Indoor Environments.</span></p>
<p class=MsoNormal style='margin-left:1.0cm;text-indent:0cm'><span lang=EN-GB style='font-size:13.0pt;font-family:"Cambria","serif"'>&nbsp;</span></p>

<p>&nbsp;</p>
<p align="justify">  © Copyright 2012 Authors - This is an Open Access article published under the <a href="http://creativecommons.org/licenses/by/2.0" target="_blank">Creative Commons Attribution License terms</a>. Unrestricted use, distribution, and reproduction in any medium are permitted, provided the original work is properly cited.</p>


<img src="../images/line.png" width="100%;" height="1" />
<h2>1. Introduction</h2> 


<p align="justify" style="text-indent:19.85pt;">Although many improvements have been done in public roads and buildings, it is still hard and dangerous for the visually impaired people to navigate in most of the places.  Several approaches have been proposed to address this problem, which are mainly divided into two groups: devices for (1) outdoor and (2) indoor environments. Most of the devices for outdoor environments are based on the Global Positioning System (GPS) (Mori, Kotani, (1998), Brusnighan (1989), Loomis et al., (2001), Makino (1997)) for localization and Laser Range Finder (LRF) for obstacle avoidance. For example, Mori and Kotani (1989) proposed a Robotic Travel Aid (RoTA) "Harunobu" to guide the visually impaired people in the sidewalk or university campus using a camera, sonar, differential GPS system, and portable GIS, which are placed in an actuated wheelchair device. On the other hand, indoor devices use ultrasound or radio frequency identification transponder for localization and LRF for obstacle identification and avoidance (Ueda et al., (2006), Helal, A. (2001), Hesch and Roumeliotis (2007), Kulyukin (2004), Na (2006), Broadbent et al (2009), Salvini (2010)). For example, Ueda et al. (2006) proposed a new system, which gives blind people information of the environment. The person is equipped with two LRF, PC placed in his body. </p>

<p align="justify" style="text-indent:19.85pt;">In order to determine the most important information needed by visually impaired people, we interviewed several of them in the Toyama Center of Disabled People. The survey results showed that especially steps and stairs present a great danger for these people. In addition, guiding these persons to get to the desired locations in indoor environments such as hospitals, city office etc, is also important. On the basis of these problems, we propose a novel robotic system using camera and laser range finders. The sensors and a small PC are placed in a trolley walker. While walking, the camera captures the images and the LRFs scan the environment in front of the user in the horizontal and vertical planes. The PC analyzes environmental data to acquire the information. The PC tells this information to the person by natural language or beep signals. By using this robotic system, visually impaired people will be able to obtain information of the environment, find obstacle free moving directions and danger places like stairs and steps. In difference from previously proposed systems, the robot operate not only in the assist mode but also in guide mode. In our previous method (Capi 2012), we a black color line was placed along the corridor. Red and green color landmarks indicated the door on the left and right side, respectively. The robot follows the black line and stops at the target room using the camera. However, people or obstacles in front of the robot made the navigation difficult and time consuming. In order for the guide robot to navigate in dynamic and non-stationary environments, we evolved neural controllers enabling the robot to move the target location. </p>

<p align="justify" style="text-indent:19.85pt;"> This paper is organized as follows. First the developed system is described (section II). In Section III the software for assistive and guide modes are presented. The performance of our proposed system in different environment settings is verified experimentally and the results are presented in section IV, before concluding in Section V. </p>

<h2>2. Developed System</h2>

<p align="justify" style="text-indent:19.85pt;">The proposed robotic system is shown in Fig. 1. It consists of a PC, two LRFs, one camera, and two Yamaha AC motors to drive the right and left wheels. The PC and sensors are placed in a trolley walker produced in collaboration with Kanayama Machinery. The laser sensors scan the environment in front of the trolley walker, in the horizontal and vertical planes, while the camera captures the environment images. The PC analyzes the sensors data continuously. The PC detects objects that are in front of the robot and tells the user this information by beep signals or natural language. In our system, we use a bone headphone which makes it possible for the user to listen to PC information, while hearing the environment. The intensity of the beep signals increases as the guide robot gets nearer to the obstacles.</p>

<p align="justify" style="text-indent:19.85pt;">In our system, we used two LRF sensors made by HOKUYO AUTOMATIC. The URG-04LX-UG01 (Ueda et al., (2006)) is a small and light LRF sensor. Its high accuracy, high resolution and wide range provide a good solution for 2D scanning. Fig. 2 shows the appearance of the sensor and Table 1 gives the main specifications. It has a low-power consumption (2.5W) for longer working time and can be powered by the USB bus.</p>

<figure>
<img src="002_files/image001.jpg" class="img"/>

<figcaption>
Figure 1. Developed system. </figcaption>
</figure>
<br>

<figure>
<img src="002_files/image002.jpg" class="img"/>

<figcaption>
Figure 2. Hokuyo Laser Range Finder.</figcaption>
</figure>
<br>


<div class="widetable">
<table>
<caption>Table 1. Laser
range finder specifications</caption>
<thead>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Detectable distance</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border:solid black 1.0pt;
  border-left:none;padding:0cm 5.4pt 0cm 5.4pt'>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>20 to 5600mm</p>
  </td>
 </tr>
 </thead>
 <tbody>
 <tr>
  <td width=169 style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Accuracy</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>60 to 1,000mm: ± 30mm</p>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>1,000 to 4,095mm: ±3 %</p>
  </td>
 </tr>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Resolution</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>1mm</p>
  </td>
 </tr>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Scanning angle</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>240[degrees]</p>
  </td>
 </tr>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Angular resolution</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>0.36°(360°/1,024 steps)</p>
  </td>
 </tr>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Scanning time</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>100ms/scan</p>
  </td>
 </tr>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>Weight Approx.</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
      <p class=MsoNormalCxSpMiddle style='text-align:center;'>Approx. 160g</p>
  </td>
 </tr>
 <tr>
  <td width=169 valign=top style='width:126.45pt;border:solid black 1.0pt;
  border-top:none;padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>External dimension</p>
  </td>
  <td width=189 valign=top style='width:141.65pt;border-top:none;border-left:
  none;border-bottom:solid black 1.0pt;border-right:solid black 1.0pt;
  padding:0cm 5.4pt 0cm 5.4pt'>
        <p class=MsoNormalCxSpMiddle style='text-align:center;'>50×50×70[mm]</p>
  </td>
 </tr>
 </tbody>
</table>
</div>

<figure>
<img src="002_files/image004.jpg" class="img"/><br>(a) Horizontal LRF<br>
<img src="002_files/image005.jpg" class="img"/><br>(b) Vertical LRF
<figcaption>
Figure 3. Beep signal area of the horizontal LRF.</figcaption>
</figure>
<br>

<figure>
<img src="002_files/image006.jpg" class="img"/>
<img src="002_files/image007.jpg" class="img"/>
<figcaption>
Figure 4. Distinction between step and stairs by LRF.</figcaption>
</figure>
<br>



<H2>3. Software</H2>
<p align="justify" style="text-indent:19.85pt;">The function of the system is roughly classified into two parts. </p>

<ul><li>Capturing and processing the camera image in real time. At this time, the main application of the visual sensor is to detect landmarks in the environment. However, we plan to utilize the camera for object and face recognition.</li>
<li>LRF data acquisition and processing for obstacle detection in the vertical and horizontal planes. By processing the LRF data, we are also able to detect steps and stairs.</li></ul>

<h4>3.1 Assistive mode</h4>

<p align="justify" style="text-indent:19.85pt;">As shown in Table 1 the maximum scanning range of the laser sensor is 4m and 240 degrees. However, in order not to disturb the user with unnecessary information, the beep signal starts when the obstacle enters in a rectangular area of (0.6m x 0.5m) in front of the LRF scanning the environment in the horizontal plane (Fig. 3a). In the vertical plane, we restrict the area to 0.5m x 1.7m (Fig. 3(b)). These parameters can be changed based on the walking speed and the height of the user. </p>

<p align="justify" style="text-indent:19.85pt;">In order to determine the most important information needed during the guidance, we interviewed visually impaired people at the Center of Disabled People in Toyama, Japan. The survey showed that stairs and steps present a great danger for disabled people. Therefore, we developed algorithms for recognizing the step and stairs using the LRF in the vertical plane. First, we process the laser data to ignore the noise. Then the data are classified into groups based on their x and y coordinates. The number of groups makes the distinction between the steps (2 groups) and the stairs (3 or more), as shown in Fig. 4 (a) and Fig. 4 (b), respectively.</p>

<h4>3.2 Guide Mode</h4>

<p align="justify" style="text-indent:19.85pt;">In the control PC, we have developed a GUI where the map of the environment is also included (Fig. 5a). Our goal is to use the system in public buildings. Therefore, we consider that the robot starts the guide motion in the same place. In such buildings, for example hospitals, the reception place or elevator is usually the starting place. When the receptionist clicks on the target room, the specific neural controller is activated to guide the used to the desired location. In order to evolve the neural controllers, we developed a simulated environment similar to the real environment in our laboratory (Fig. 5b). We evolved the neural controllers that guide the visually impaired from the initial to the target location. </p>

<p align="justify" style="text-indent:19.85pt;">In addition, during the robot motion, the LRF data are processed in order to check for moving or fixed obstacles. If humans get in front, the robot stops moving and after no object is detected the robot restarts the guidance.</p>

<figure>
<img src="002_files/image008.jpg" class="img"/><br>(a)    Developed GUI<br>
<img src="002_files/image009.jpg" class="img"/><br>(b)	Simulated environment

<figcaption>
Figure 5. Developed software for guide mode.</figcaption>
</figure>
<br>



<h4><i>3.2.1 Evolving Neural Controllers</i></h4>

<p align="justify" style="text-indent:19.85pt;">We implemented a simple feed-forward neural controller with 21, 6 and 2 units in the input, hidden and output layers, respectively (Fig. 6). The inputs of the neural controller are 20 laser sensor data distributed uniformly in 180° and the reading of the compass sensor. The laser data varies from 0 to 1, where 0 corresponds to no obstacle in the range less than 4m and 1 correspond to the obstacle in the distance 0.2m. The compass sensor data varies from 0° to 360° giving information about the robot orientation. Random noise, uniformly distributed in the range of +/- 5% of sensor readings, has been added to the compass sensor. The hidden and output units use sigmoid activation function:</p>

<div align=center>


<table class=MsoTableGrid border=0 cellspacing=0 cellpadding=0 width="100%"
 style='width:100.0%;border-collapse:collapse;border:none'>
 <tr>
  <td width="100%" style='width:91.46%;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=MsoNormal align=center style='text-align:center'><span
  style='font-size:11.0pt;'><img src="002_files/Eqn001.gif" class="img" width="100"></span></p>
  </td>
  <td width="10%" style='width:8.54%;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=MsoNormal align=right style='text-align:right'>(1)</p>
  </td>
 </tr>
</table>


</div>


<p align="justify" style="text-indent:19.85pt;">where the incoming activation for node i is:</p>


<div align=center>


<table class=MsoTableGrid border=0 cellspacing=0 cellpadding=0 width="100%"
 style='width:100.0%;border-collapse:collapse;border:none'>
 <tr>
  <td width="100%" style='width:91.46%;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=MsoNormal align=center style='text-align:center'><span
  style='font-size:11.0pt;'><img src="002_files/Eqn002.gif" class="img"></span></p>
  </td>
  <td width="10%" style='width:8.54%;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=MsoNormal align=right style='text-align:right'>(2)</p>
  </td>
 </tr>
</table>


</div>


<p align="justify" style="text-indent:19.85pt;">and j ranges over nodes with weights into node i.</p>

<p align="justify" style="text-indent:19.85pt;">The output units directly control the robot moving direction where 0 corresponds to full right turn and 1 corresponds to full left turn. The robot moving velocity is considered to be constant (0.5 m/s).</p>

<p align="justify" style="text-indent:19.85pt;">GA operates through a simple cycle of four stages. Each cycle produces a new generation of possible solutions for a given problem. At the first stage, an initial population of potential solutions is created as a starting point for the search. In the next stage, the performance (fitness) of each individual is evaluated with respect to the constraints imposed by the problem. Based on each individual's fitness, a selection mechanism chooses "parents" for the crossover and mutation operators.</p>

<p align="justify" style="text-indent:19.85pt;">The crossover operator takes two chromosomes and swaps part of their genetic information to produce new chromosomes. The mutation operator introduces new genetic structures in the population by randomly modifying some of the genes, helping the search algorithm to escape from local minima's traps. The offsprings produced by the genetic manipulation process are the next populations to be evaluated. GA can replace either a whole population or its less fitted members only. The creation- evaluation- selection-manipulation cycle repeats until a satisfactory solution to the problem is found or some other termination criteria are met.</p>

<p align="justify" style="text-indent:19.85pt;">For any evolutionary computation technique, a chromosome representation is needed to describe each individual in the population. In our implementation, the genome of every individual of the population encodes the weight connections of the neural controller. The genome length is 138 and the connection weights range from -5 to 5. Each individual of the population controls the Guide Robot during its lifetime. At the end, the fitness function is calculated. The fitness function selects robots for their ability to move to the goal location, following a trajectory which does not get near to the obstacles and walls. Therefore, the fitness is as follows:</p>

<div align=center>


<table class=MsoTableGrid border=0 cellspacing=0 cellpadding=0 width="100%"
 style='width:100.0%;border-collapse:collapse;border:none'>
 <tr>
  <td width="100%" style='width:91.46%;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=MsoNormal align=center style='text-align:center'><span
  style='font-size:11.0pt;'><i>fitness</i>= 2*<i>step_nr</i> + <i>obj_dist</i> + 250<i>SenData/step_nr</i></span></p>
  </td>
  <td width="10%" style='width:8.54%;padding:0cm 5.4pt 0cm 5.4pt'>
  <p class=MsoNormal align=right style='text-align:right'>(3)</p>
  </td>
 </tr>
</table>


</div>


<p align="justify" style="text-indent:19.85pt;">where step_nr is the number of steps to get to the target location, obj_dist is the distance to the target if the robot hits obstacles, and SenData is the difference between the right and left laser data in absolute value summed during all robot motion.</p>

<h2>4. Experimental Results</h2>
<h4>4.1 Assistive mode</h4>

<p align="justify" style="text-indent:19.85pt;">In order to evaluate the performance of the proposed system, several experimental implementations are conducted at the University of Toyama's Intelligent Robotics Laboratory. The performance analysis is done for obstacle, step and stairs detection and for finding the obstacle free paths in the environment. Fig. 7(a) shows the environment picture and plot of LRF data scanning the environment horizontally in a situation where the obstacle is on the right side. In this case, the robotic system informs the user about the presence of the obstacle by natural language as follows: "Be careful on the right". Fig 7(b) shows the tabletops like obstacle detected by robotic system using the LRF scanning the environment in the vertical plane. By using the vertical LRF, we are also able to detect the steps and stairs. Fig. 7(c) shows that although the relative angle between the stairs and the robotic system is very small, the laser data discontinuity makes it possible to detect the stairs. The robotic system informs the user as soon as the stairs or steps are detected, saying "stairs, stairs" or "steps, steps".</p>


<figure>
<img src="002_files/image018.jpg" class="img"/>

<figcaption>
Figure 6. Structure of neural controller.</figcaption>
</figure>
<br>

<figure>
<img src="002_files/image019.gif" class="img"/><br>(a) Obstacle in front recognized using the horizontal scanning laser<br>
<img src="002_files/image020.gif" class="img"/><br>(b) Tabletops like obstacle recognized using the vertical scanning laser 3<br>
<img src="002_files/image021.gif" class="img"/><Br>(c) Stairs recognition using the vertical scanning laser

<figcaption>
Figure 7. Laser readings in different situations.</figcaption>
</figure>
<br>


<figure>
<img src="002_files/image022.jpg" class="img"/>

<figcaption>
Figure 8. Average of stopping distance for stairs and steps (10 subjects)</figcaption>
</figure>
<br>

<figure>
<img src="002_files/image023.jpg" class="img"/>

<figcaption>
Figure 9. Simulation results.</figcaption>
</figure>
<br>

<p align="justify" style="text-indent:19.85pt;">The performance of the robotic system for stair recognition is tested in three different relative angles between the trolley walker and the stairs (90, 70 and 50 degrees). Not surprisingly, the results show that best recognition rate was when the robotic system was perpendicular with the stairs or step. The results in Fig. 8 show the stopping distance from the trolley walker to the stairs and steps of ten different subjects. The width and height of the stair (step) was 0.3m and 0.2m, respectively. The average stopping distance was 70.6cm for the stairs and 71.7cm for the steps.</p>

<H4>4.2 Assistive mode</H4>

<p align="justify" style="text-indent:19.85pt;">Simulation results for guide mode are shown in Fig. 9 where the target locations are rooms A and B (Fig. 5a). The robot starts motion in front of the elevator. The guide robot is controlled by the best evolved neural controller. Due to the design of the fitness function which selects neural controllers that keep the robot far from the obstacles while moving to the target location, the robot makes a fast left turn when gets near to the target location. The robot always stopped at the target door. Now, we are working to evolve neural networks able to control the guide robot in more unstructured environments where there are unknown fixed and moving obstacles. In addition, we are implementing the evolved neural controllers in the real hardware of the guide robot. </p>

<h2>5. Conclusion</h2>

<p align="justify" style="text-indent:19.85pt;">This paper proposed a robotic system that can assist and guide the visually impaired in indoor environments. The mobile robotic system, equipped with two small laser range finders and camera. As confirmed by experimental results, the system is able to recognize obstacles, steps and stairs. In the guide mode, the robot is controlled by evolved neural networks generated in a simulated environment. Therefore, the robot was able to navigate to the target location avoiding hitting the obstacles. </p>

<p align="justify" style="text-indent:19.85pt;">In the future, we plan to increase the number of sensors and to apply the robot in outdoor environments. We plan to use a moving camera able to detect cars or moving objects coming from the left and right sides. Another improvement of the system will be the GPS system and application in outdoor environments.</p>

<h2>Acknowledgements</h2>

<p align="justify" style="text-indent:19.85pt;">This work is supported by Toyama New Industrial Organization, The Hokuriku Industrial Advancement Center and the Kanayama Machinery Company.</p>

<img src="images/line.png" width="100&#37;" height="1" />
<H2><a name="references">References</a></H2>

<p> Broadbent, E., Stafford, R. MacDonald, B. (2009) "Acceptance of Healthcare Robots for the Older Population: Review and Future Directions", International Journal of Social Robotics, Vol. 1, No 4, pp. 319-330. <a href="http://dx.doi.org/10.1007/s12369-009-0030-6" target="_blank">View Article</a></p>


<p> Brusnighan, D.A. (1989). "Orientation aid implementing the global positioning system "15th IEEE Annual Northeast Bioengineering Conference", pp.33-34.   <a href="http://dx.doi.org/10.1109/NEBC.1989.36684" target="_blank">View Article</a></p>


<p> Capi, G. (2012) "Development of e New Robotic System for Assisting and Guiding Visually Impaired People", 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO 2012). <a href="http://dx.doi.org/10.1007/s12369-011-0103-1" target="_blank">View Article</a></p>


<p> Helal, A. (2001) "Drishti: an integrated navigation system for the visually impaired "IEEE  International Symposium on Wearable Computers", pp.149-156. <a href="http://dx.doi.org/10.1109/ISWC.2001.962119 " target="_blank">View Article</a></p>


<p> Hesch J.A., Roumeliotis, S.I. (2007) An Indoor localization aid for the visually impaired, "IEEE International Conference on Robotics and Automation", Roma, Italy, April 10-14, pp.3545-3551.  <a href="http://dx.doi.org/10.1109/ROBOT.2007.364021" target="_blank">View Article</a></p>


<p> Kulyukin, V., Gharpure, C. Nicholson, J. Pavithran, S. (2004) RFID in robot assisted indoor navigation for the visually impaired "IEEE International Conference on Intelligent Robots and Systems", Sandai, Japan, Sept. 28 – Oct. 2, vol. 2, pp. 1979-1984.  <a href="http://dx.doi.org/10.1109/IROS.2004.1389688" target="_blank">View Article</a></p>


<p> Loomis, J., Golledge, RG., Klatzky, R.L. (2001) GPS-based navigation systems for the visually impaired, Fundamentals of Wearable Computers and Augmented Reality, pp.429-446.  <a href="http://www.psych.ucsb.edu/~loomis/loomisgolledgeklatzky01.pdf" target="_blank">View Article</a></p>


<p> Makino, H. (1997). Development of navigation system for the blind using GPS and mobile phone combination, "IEEE International Conference of Engineering in Medicine and Biology", vol.2, pp.506-507.  <a href="http://dx.doi.org/10.1109/IEMBS.1996.651838" target="_blank">View Article</a></p>


<p> 
Mori, H., Kotani, S. (1998). Robotic Travel Aid for the Blind: HARUNOBU-6 "Second European Conference on Disability, Virtual Reality, and Assistive Technology". 
 <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.5100" target="_blank">View Article</a></p>


<p> Na, J. (2006). The blind interactive guide system using RFID-based indoor positioning system "10th International Conference on Computers Helping People with Special Needs", Linz, Austria, July 11-13, Lecture Notes in Computer Science, Vol.4061, Springer Berlin, pp.1298-1305.  <a href="http://dx.doi.org/10.1007/11788713_187" target="_blank">View Article</a></p>


<p> Salvini, P., Laschi, C., Dario, P. (2010) Design for Acceptability: Improving Robots' Coexistence in Human Society, International Journal Social Robotics 2(4), pp. 451-460. <a href="http://dx.doi.org/10.1007/s12369-010-0079-2" target="_blank">View Article</a></p>


<p> Ueda, T., Kawata, H., Tomizawa, T., Ohya A., Yuta, S. (2006) Mobile SOKUIKI Sensor System - Accurate Range Data Mapping System with Sensor Motion, "Proceedings of the 2006 International Conference on Autonomous Robots and Agents". <a href="www.tomy3.com/Research/ICARA2006-UED.pdf " target="_blank">View Article</a></p>


<p> Ueda, T., Kawata, H., Tomizawa, T., Ohya A., Yuta, S. (2006) Visual Information Assist System Using 3D SOKUIKI Sensor for Blind People -System Concept and Object Detecting Experiments, IECON'06 The 32nd Annual Conference of the IEEE Industrial Electronics Society Proceedings, pp.3058-30636. <a href="http://dx.doi.org/10.1109/IECON.2006.347767" target="_blank">View Article</a></p>
<div class="clearfix"></div>
  </div>
<div class="clearfix"></div>
  </div>
<div class="clearfix"></div>
  </div>

  

  <div class="footer">

  <div class="container">

  <div class="col-sm-7 footer-link">

        <p><a href="http://avestia.com">Avestia Publishing</a></p>
        <p><a href="../register/">Subscribe to our Mailing List</a></p>
        <p><script>var refURL = window.location.protocol + "//" + window.location.host + window.location.pathname; document.write('<a href="http://avestia.com/feedback?refURL=' + refURL+'">Feedback</a>');</script></p>
        <p><a href="../terms/">Terms of Use</a></p>
		<p><a href="../sitemap/">Sitemap</a></p>

      </div>
      <div class="col-sm-5 footer-txt">
        <p>Avestia Publishing, International ASET Inc. </p>
        <p>Unit No. 417, 1376 Bank St. </p>
        <p>Ottawa, Ontario, Canada </p>
        <p>Postal Code: K1H 7Y3</p>
        <p>Phone Number: 1-613-695-3040</p>

  </div>

  </div>

  </div>

</div>
<script src="../js/jquery.js"></script>
<script src="../js/bootstrap.js"></script>
</body>

</html>

